{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d166c6f-8705-47c7-8af5-91e6a6699e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import regex as re\n",
    "from packaging import version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4baa69cf-99c9-4d1b-bf26-eb5b6b991a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to check if SLEAP is the new or old version on the device.\n",
    "\n",
    "# This checks the conda version of the SLEAP being used since apparently they changed the confidence of a point from .p to .score.\n",
    "# Potential Issue: I haven't checked if this works on old SLEAP envrionmnts \n",
    "# result = subprocess.run(\n",
    "#     ['conda', 'run', '-n', 'sleap', 'python', '-c', 'import sleap; sleap.versions()'],\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "# print(type(result.stdout))\n",
    "\n",
    "\n",
    "\n",
    "# match = re.search(r'SLEAP:\\s*([\\d.]+)', result.stdout)\n",
    "# if match:\n",
    "#     sleap_version = match.group(1)\n",
    "#     print(sleap_version)  # 1.5.2\n",
    "\n",
    "# if match:\n",
    "#     sleap_version = match.group(1)\n",
    "    \n",
    "#     # Compare versions\n",
    "#     if version.parse(sleap_version) >= version.parse(\"1.4.1\"):\n",
    "#         print(f\"SLEAP {sleap_version} is >= 1.4.1\")\n",
    "#         SLEAP_NEW = True  \n",
    "#     else:\n",
    "#         print(f\"SLEAP {sleap_version} is < 1.4.1\")\n",
    "#         SLEAP_OLD = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca4e1e6-a6eb-497c-9c29-74dd82ec00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SLEAP_NEW:\n",
    "#     body_coords = ['.x','.y','.score']\n",
    "# else:\n",
    "#     body_coords = ['.x','.y','.p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cd5e4e-aa7f-4b78-9ac2-4bea80de693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a search script that will find the files that match a pattern in the directory and return said matches\n",
    "\n",
    "# Rootdir= \"C:/MolasLAB/simba_training_videos/Predictions_file/unclean_predictions\" # replace this the scope of the search\n",
    "# OutputPath=\"C:/MolasLAB/simba_training_videos/Predictions_file/cleaned_predictions\" # os.getcwd() # can be replaced with a dedicated path ie 'C:/Users/Jume6823.AD.000/Downloads'\n",
    "\n",
    "# Rootdir= \"C:/MolasLAB/simba_training_videos/Predictions_file/unclean_predictions\" # replace this the scope of the search\n",
    "Rootdir =r\"C:\\Users\\jjmmc\\Documents\\no_cleaning\"\n",
    "ALL_FILES_IN_ROOT = True\n",
    "#SinglePattern= \"L\\d{1}\\sTrap2\\d{3}.analysis$\"\n",
    "#PairPattern = \"L\\d{1}\\sTrap2\\d{3}[+]\\d{3}.analysis$\"\n",
    "\n",
    "def CSVSearch(RootDir, pair:bool,  pattern_on  = False):\n",
    "    if pattern_on == True:\n",
    "        if pair == False:\n",
    "            pattern= re.compile(r'L\\d{1}\\sTrap2\\d{3}.analysis')\n",
    "        else:\n",
    "            pattern= re.compile(r'L\\d{1}\\sTrap2\\d{3}[-+]\\d{3}.analysis')\n",
    "    \n",
    "    matching = []\n",
    "    for item in os.listdir(RootDir):\n",
    "        filepath = os.path.join(RootDir, item).replace(\"\\\\\", \"/\")\n",
    "        if os.path.isfile(filepath):  # Only include files, not directories\n",
    "            matching.append(filepath)\n",
    "    return(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1597cb-e4a7-4f6a-8567-e7d0376e3fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_205+206.csv', 'C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_208+209.csv', 'C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_218+219.csv', 'C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_205+206.csv', 'C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_208+209.csv', 'C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_218+219.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if ALL_FILES_IN_ROOT:\n",
    "    Doubles = CSVSearch(Rootdir, False)\n",
    "# Doubles= [r\"C:\\Users\\jjmmc\\Downloads\\simple_social_test_csv.csv\"]\n",
    "\n",
    "print(Doubles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f631ad6-3984-44d4-a485-7ac3b025658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_coords = ['.x','.y','.score']\n",
    "\n",
    "def identity_swap_tracker(animal_1_df, animal_2_df, body_parts):\n",
    "    \"\"\"Detect potential identity swaps between two tracked animals based on body part positions.\"\"\"\n",
    "    shifted_1 = animal_1_df.shift(1).bfill()\n",
    "    shifted_2 = animal_2_df.shift(1).bfill()\n",
    "\n",
    "    summed_euclidean_1 = np.zeros(len(animal_1_df))\n",
    "    summed_euclidean_2 = np.zeros(len(animal_1_df))\n",
    "    summed_euclidean_1_to_2 = np.zeros(len(animal_1_df))\n",
    "    summed_euclidean_2_to_1 = np.zeros(len(animal_1_df))\n",
    "\n",
    "    for part in body_parts:\n",
    "        animal1_coords = animal_1_df.filter(like=part).to_numpy()\n",
    "        animal2_coords = animal_2_df.filter(like=part).to_numpy()\n",
    "        shifted1_coords = shifted_1.filter(like=part).to_numpy()\n",
    "        shifted2_coords = shifted_2.filter(like=part).to_numpy()\n",
    "\n",
    "        dist_1 = np.linalg.norm(shifted1_coords - animal1_coords, axis=1)\n",
    "        dist_2 = np.linalg.norm(shifted2_coords - animal2_coords, axis=1)\n",
    "        dist_1_to_2 = np.linalg.norm(shifted1_coords - animal2_coords, axis=1)\n",
    "        dist_2_to_1 = np.linalg.norm(shifted2_coords - animal1_coords, axis=1)\n",
    "\n",
    "        summed_euclidean_1 += dist_1\n",
    "        summed_euclidean_2 += dist_2\n",
    "        summed_euclidean_1_to_2 += dist_1_to_2\n",
    "        summed_euclidean_2_to_1 += dist_2_to_1\n",
    "    potential_swap_1_to_2 = summed_euclidean_1_to_2 < summed_euclidean_1\n",
    "    potential_swap_2_to_1 = summed_euclidean_2_to_1 < summed_euclidean_2\n",
    "\n",
    "    swapped_indices = np.where(potential_swap_1_to_2 & potential_swap_2_to_1)[0] \n",
    "    return swapped_indices\n",
    "\n",
    "\n",
    "def collision_detector(df0, df1, body_parts, pairwise_threshold=10, continuity_threshold=50):\n",
    "    \"\"\"\n",
    "    This detects if two skeletons are too close and therefore not distinct which can cause big issues for interpolation.\n",
    "    Notably this uses the set of pairwise differences to determine this with the criteria being if the two seketons effectively share 3 or. This is since in ideal pose data one can have 1-2 parts reasonably be in vey close proximity (ie the noses or the ears)\n",
    "    This returns the dataframes with these problematic frames removed. \n",
    "    \"\"\"\n",
    "\n",
    "    df0 = df0.copy().reset_index(drop=True)\n",
    "    df1 = df1.copy().reset_index(drop=True)\n",
    "    N = len(df0)\n",
    "\n",
    "    # 1. Convert the pandas data to numpy\n",
    "    \n",
    "    # For each bodypart create vectors of shape (N, num_parts*2)\n",
    "    coords0 = np.column_stack([df0[f\"{bp}.x\"].to_numpy() for bp in body_parts] +\n",
    "                              [df0[f\"{bp}.y\"].to_numpy() for bp in body_parts])\n",
    "    coords1 = np.column_stack([df1[f\"{bp}.x\"].to_numpy() for bp in body_parts] +\n",
    "                              [df1[f\"{bp}.y\"].to_numpy() for bp in body_parts])\n",
    "\n",
    "    num_parts = len(body_parts)\n",
    "\n",
    "    # Split x/y back out for pairwise distance operations\n",
    "    x0 = coords0[:, :num_parts]\n",
    "    y0 = coords0[:, num_parts:]\n",
    "    x1 = coords1[:, :num_parts]\n",
    "    y1 = coords1[:, num_parts:]\n",
    "\n",
    "    # 2. Calculate the distance between identites with respect to the same frame\n",
    "    pairwise_dist = np.sqrt((x0 - x1)**2 + (y0 - y1)**2)   # shape (N, num_parts)\n",
    "\n",
    "    # Count parts whose distances exceed threshold\n",
    "    separated_count = np.nansum(pairwise_dist > pairwise_threshold, axis=1)\n",
    "\n",
    "    # Collision mask: < 3 separated bp\n",
    "    coll_mask = separated_count < 3\n",
    "    coll_mask[0] = False  # first frame cannot be evaluated\n",
    "\n",
    "    # 3. Compure the framewise differences of the identities, \n",
    "\n",
    "    # Shifted (t-1) frames\n",
    "    x0_prev = np.roll(x0, 1, axis=0)\n",
    "    y0_prev = np.roll(y0, 1, axis=0)\n",
    "    x1_prev = np.roll(x1, 1, axis=0)\n",
    "    y1_prev = np.roll(y1, 1, axis=0)\n",
    "\n",
    "    # This then corrects the wrapping that comes as a result of rolling\n",
    "    x0_prev[0] = x0_prev[1]\n",
    "    y0_prev[0] = y0_prev[1]\n",
    "    x1_prev[0] = x1_prev[1]\n",
    "    y1_prev[0] = y1_prev[1]\n",
    "\n",
    "    # Euclidean norm summed across parts\n",
    "    def partwise_dist(ax, ay, bx, by):\n",
    "        return np.nansum(np.sqrt((ax - bx)**2 + (ay - by)**2), axis=1) #nansum accounts for missing data as treated it as 0 \n",
    "\n",
    "    d00 = partwise_dist(x0, y0, x0_prev, y0_prev)\n",
    "    d11 = partwise_dist(x1, y1, x1_prev, y1_prev)\n",
    "    # d01 = partwise_dist(x0, y0, x1_prev, y1_prev)\n",
    "    # d10 = partwise_dist(x1, y1, x0_prev, y0_prev)\n",
    "\n",
    "    # Continuity masks\n",
    "    cont00_bad = d00 > continuity_threshold\n",
    "    cont11_bad = d11 > continuity_threshold\n",
    "    # cont01_bad = d01 > continuity_threshold\n",
    "    # cont10_bad = d10 > continuity_threshold\n",
    "\n",
    "    \n",
    "    # 4. Condtions for identity/frame removal\n",
    "    \n",
    "    # Remove both if both identity and swapped continuity fails.\n",
    "    remove_both = (cont00_bad & cont11_bad & coll_mask)\n",
    "    \n",
    "    # remove_both = (cont00_bad & cont11_bad & cont01_bad & cont10_bad & coll_mask)\n",
    "\n",
    "    #  Otherwise remove least continuous identity \n",
    "    #     Compare identity vs swapped continuity scores.\n",
    "    #     Use raw distances to pick the larger (= worse).\n",
    "    \n",
    "    # cont_id0 = np.minimum(d00, d01)   # best mapping for ID0\n",
    "    # cont_id1 = np.minimum(d11, d10)   # best mapping for ID1\n",
    "\n",
    "    remove_0 = (d00 > d11) & coll_mask & (~remove_both)\n",
    "    remove_1 = (d11 > d00) & coll_mask & (~remove_both)\n",
    "    # remove_0 = (cont_id0 > cont_id1) & coll_mask & (~remove_both)\n",
    "    # remove_1 = (cont_id1 > cont_id0) & coll_mask & (~remove_both)\n",
    "    removed_indices = {\n",
    "        'identity_0': np.where(remove_0)[0].tolist(),\n",
    "        'identity_1': np.where(remove_1)[0].tolist(),\n",
    "        'both': np.where(remove_both)[0].tolist()\n",
    "    }\n",
    "    # 5. Remove the bad identites \n",
    "    cols = [f\"{bp}{ax}\" for bp in body_parts for ax in body_coords]\n",
    "\n",
    "    df0.loc[remove_0 | remove_both, cols] = np.nan\n",
    "    df1.loc[remove_1 | remove_both, cols] = np.nan\n",
    "  \n",
    "    return df0, df1, removed_indices\n",
    "\n",
    "def gap_identity_tracker(df0, df1, body_parts):\n",
    "    \"\"\"\n",
    "    Detects potential swaps over gaps where one or both identities are missing.\n",
    "    Returns a list of (start_frame, end_frame) tuples where a swap should be applied.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = df0['frame_idx'].to_numpy()\n",
    "    exist0 = df0.dropna(subset=[f\"{bp}.x\" for bp in body_parts])['frame_idx'].to_numpy()\n",
    "    exist1 = df1.dropna(subset=[f\"{bp}.x\" for bp in body_parts])['frame_idx'].to_numpy()\n",
    "\n",
    "    missing_both_mask = ~(np.isin(all_frames, exist0) | np.isin(all_frames, exist1))\n",
    "    swapped_intervals = []\n",
    "\n",
    "    # Split missing frames into consecutive gaps\n",
    "    double_gaps = np.split(all_frames[missing_both_mask], np.where(np.diff(all_frames[missing_both_mask]) != 1)[0]+1)\n",
    "    for gap in double_gaps:\n",
    "        if len(gap) == 0:\n",
    "            continue\n",
    "\n",
    "        frame_start = gap[0] - 1\n",
    "        frame_end = gap[-1] + 1\n",
    "\n",
    "        # Get last known positions before the gap\n",
    "        last0 = df0[df0['frame_idx'] <= frame_start].tail(1)\n",
    "        last1 = df1[df1['frame_idx'] <= frame_start].tail(1)\n",
    "\n",
    "        # Get first known positions after the gap\n",
    "        next0 = df0[df0['frame_idx'] >= frame_end].head(1)\n",
    "        next1 = df1[df1['frame_idx'] >= frame_end].head(1)\n",
    "\n",
    "        if last0.empty or last1.empty or next0.empty or next1.empty:\n",
    "            continue  # Can't decide swap if boundary data missing\n",
    "\n",
    "        # Calculate distances if identities are swapped\n",
    "        dist_no_swap = 0\n",
    "        dist_swap = 0\n",
    "        for bp in body_parts:\n",
    "            for suffix in ['.x', '.y']:\n",
    "                last0_val = last0[f\"{bp}{suffix}\"].values[0]\n",
    "                last1_val = last1[f\"{bp}{suffix}\"].values[0]\n",
    "                next0_val = next0[f\"{bp}{suffix}\"].values[0]\n",
    "                next1_val = next1[f\"{bp}{suffix}\"].values[0]\n",
    "\n",
    "                dist_no_swap += (next0_val - last0_val)**2 + (next1_val - last1_val)**2\n",
    "                dist_swap += (next0_val - last1_val)**2 + (next1_val - last0_val)**2\n",
    "\n",
    "        if dist_swap < dist_no_swap:\n",
    "            # store the last frame \n",
    "            swapped_intervals.append( gap[-1])\n",
    "    print(swapped_intervals)\n",
    "    return swapped_intervals\n",
    "\n",
    "\n",
    "def apply_cumulative_swaps(df0, df1, swap_points):\n",
    "    \"\"\"\n",
    "    Apply cumulative identity swaps between two animal tracking DataFrames.\n",
    "    Each swap point indicates the frame index *after which* identities flip.\n",
    "    Swapping twice restores the original identity, so only the parity matters.\n",
    "    \"\"\"\n",
    "    df0 = df0.copy()\n",
    "    df1 = df1.copy()\n",
    "\n",
    "    # Sort and deduplicate swap points\n",
    "    swap_points = np.sort(np.unique(swap_points))\n",
    "    if len(swap_points) == 0:\n",
    "        return df0, df1\n",
    "\n",
    "    frames = df0['frame_idx'].to_numpy()\n",
    "    # Determine whether each frame should be flipped (odd number of swaps so far)\n",
    "    flip_mask = np.cumsum(np.isin(frames, swap_points)) % 2 == 1\n",
    "    \n",
    "    # Get columns to swap (exclude frame_idx and track)\n",
    "    cols_to_swap = [col for col in df0.columns if col not in ['frame_idx', 'track']]\n",
    "    \n",
    "    # Apply swap only to coordinate columns\n",
    "    temp = df0.loc[flip_mask, cols_to_swap].copy()\n",
    "    df0.loc[flip_mask, cols_to_swap] = df1.loc[flip_mask, cols_to_swap].values\n",
    "    df1.loc[flip_mask, cols_to_swap] = temp.values\n",
    "\n",
    "    return df0, df1\n",
    "\n",
    "\n",
    "\n",
    "def double_identity_tracker(index):\n",
    "    \"\"\"\n",
    "    Reads tracking data from Doubles[index], separates by identity,\n",
    "    detects and corrects identity swaps (both normal and gap-based),\n",
    "    interpolates missing data, and saves the cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    #  Step 1: Load CSV and change the columns if from a newer version of SLEAP\n",
    "    df = pd.read_csv(Doubles[index], delimiter=',', quotechar='|')\n",
    "    df.rename(columns=lambda x: x.replace('.p', '.score'), inplace=True)\n",
    "    print(f\"Loaded {len(df)} rows from {Doubles[index]}\")\n",
    "\n",
    "    #  Step 2: Parse IDs and frame numbers \n",
    "    df['track'] = df['track'].astype(str).str.extract(r'(\\d+)$').astype(int)\n",
    "    df['frame_idx'] = df['frame_idx'].astype(int)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    #  Step 3: Define frame range \n",
    "    min_frame = 0\n",
    "    max_frame = df['frame_idx'].iloc[-1]\n",
    "    frame_range = np.arange(min_frame, max_frame + 1)\n",
    "\n",
    "    #  Step 4: Identify body parts and coordinates\n",
    "    body_parts = [\"nose\", \"ear_left\", \"ear_right\", \"head\", \"body_center\", \"tail_base\"]\n",
    "    \n",
    "    #  Step 5: Split into identity DataFrames \n",
    "    identities = sorted(df['track'].unique())\n",
    "    if len(identities) != 2:\n",
    "        print(identities)\n",
    "        raise ValueError(\"This function currently supports exactly two identities.\")\n",
    "        \n",
    "\n",
    "    tracks = {}\n",
    "    for tid in identities:\n",
    "        tdf = df[df['track'] == tid].set_index('frame_idx').reindex(frame_range).reset_index()\n",
    "        tdf['track'] = tid\n",
    "        tracks[tid] = tdf\n",
    "\n",
    "    print(f\"Created {len(tracks)} identity tracks with {len(frame_range)} frames each\")\n",
    "    \n",
    "    df0, df1 = tracks[identities[0]], tracks[identities[1]]\n",
    "    # Step 5.5: Remove overlapped identities\n",
    "    df0, df1, collisions = collision_detector(df0, df1, body_parts)\n",
    "    # Step 6: Detect gap swaps and fix BEFORE interpolation \n",
    "    print(\"Detecting gap-based identity swaps...\")\n",
    "    swap_gaps = gap_identity_tracker(df0, df1, body_parts)  # returns list of intervals [(start, end), ...]\n",
    "    df0, df1 = apply_cumulative_swaps(df0, df1, swap_gaps)\n",
    "\n",
    "    # Step 7: Interpolate missing coordinates AFTER gap swaps \n",
    "    for tdf in [df0, df1]:\n",
    "        for bp in body_parts:\n",
    "            for suffix in body_coords:\n",
    "                col = next((c for c in tdf.columns if bp + suffix in c), None)\n",
    "                if col and col in tdf.columns:\n",
    "                    tdf[col] = tdf[col].interpolate(limit_direction='both').astype(float)\n",
    "    df0['track'],df1['track']= df0['track'][0], df1['track'][0] # This is fixing the track identities so that they are interpreted properly by the next function\n",
    "\n",
    "    \n",
    "    #  Step 8: Detect gap swaps and fix AFTER interpolation \n",
    "\n",
    "    print(\"Detecting normal identity swaps...\")\n",
    "    swap_normal = identity_swap_tracker(df0, df1, body_parts)  # returns individual frame indices\n",
    "    \n",
    "    df0, df1 = apply_cumulative_swaps(df0, df1, swap_normal)\n",
    "    df0['track'],df1['track']= df0['track'][0], df1['track'][0]\n",
    "    all_swaps_frames = np.sort(np.unique(np.concatenate([swap_normal] + \n",
    "                                                        [swap_gaps])))\n",
    "    print(f\"Total swaps detected: {len(all_swaps_frames)} at frames: {all_swaps_frames}\")\n",
    "\n",
    "    # df0, df1 = apply_cumulative_swaps(df0, df1, all_swaps_frames)\n",
    "\n",
    "    #  Step 9: Recombine and export \n",
    "\n",
    "    df_clean = pd.concat([df0, df1], ignore_index=True).sort_values(['frame_idx', 'track'])\n",
    "    # Move track to first position\n",
    "    df_clean.insert(0, 'track', df_clean.pop('track'))\n",
    "    \n",
    "    out_name = os.path.basename(Doubles[index])\n",
    "    \n",
    "    processed_path = os.path.join(Rootdir,'processed')\n",
    "    \n",
    "    if not os.path.exists(processed_path):\n",
    "        os.makedirs(processed_path)\n",
    "    \n",
    "    full_path = os.path.join(processed_path, out_name)\n",
    "    df_clean.to_csv(full_path, index=False)\n",
    "    \n",
    "    print(f\"Cleaned file saved as {out_name}\")\n",
    "    \n",
    "    #  Step 10: Return swap info \n",
    "\n",
    "    swap_path =os.path.join(Rootdir,'swap_data')\n",
    "    \n",
    "    if not os.path.exists(swap_path):\n",
    "        os.makedirs(swap_path)\n",
    "    \n",
    "    df_swaps = pd.DataFrame({\n",
    "        'swap_gaps': pd.Series(swap_gaps),\n",
    "        'swap_normal': pd.Series(swap_normal),\n",
    "        'collisions_id1': pd.Series(collisions['identity_0']),\n",
    "        'collisions_id2': pd.Series(collisions['identity_1']),\n",
    "        'collisions_both': pd.Series(collisions['both'])\n",
    "    })\n",
    "    \n",
    "    print(df_swaps)\n",
    "    full_swap_path= os.path.join(swap_path, f'{out_name}')\n",
    "    df_swaps.to_csv(full_swap_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        \"swapped_frames\": all_swaps_frames\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900bd796-10fd-4766-9b11-73e202cf6db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116113 rows from C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_205+206.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 identity tracks with 62689 frames each\n",
      "Detecting gap-based identity swaps...\n",
      "[np.int64(2), np.int64(2473), np.int64(2789), np.int64(5294)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 28 at frames: [    2  2191  2473  2789  4435  4769  4836  4837  5294  6450  6986 10996\n",
      " 11622 11624 11626 17073 19494 32011 39383 40474 42628 43291 43515 45422\n",
      " 48527 48534 56320 59476]\n",
      "Cleaned file saved as 20250116_TRAP2_5VLS_social_L4_TRAP2_205+206.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0           2.0       2191.0               1            48.0           2756.0\n",
      "1        2473.0       4435.0               2            50.0           2757.0\n",
      "2        2789.0       4769.0             122            51.0           7791.0\n",
      "3        5294.0       4836.0             968            54.0           9846.0\n",
      "4           NaN       4837.0             969            56.0          26259.0\n",
      "...         ...          ...             ...             ...              ...\n",
      "5056        NaN          NaN           62598             NaN              NaN\n",
      "5057        NaN          NaN           62599             NaN              NaN\n",
      "5058        NaN          NaN           62600             NaN              NaN\n",
      "5059        NaN          NaN           62601             NaN              NaN\n",
      "5060        NaN          NaN           62602             NaN              NaN\n",
      "\n",
      "[5061 rows x 5 columns]\n",
      "Loaded 106623 rows from C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_208+209.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 identity tracks with 62558 frames each\n",
      "Detecting gap-based identity swaps...\n",
      "[np.int64(1372), np.int64(5581), np.int64(6253), np.int64(13345), np.int64(22813), np.int64(30119), np.int64(35319), np.int64(35517), np.int64(35758), np.int64(41398), np.int64(55373)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 36 at frames: [  813   989  1086  1372  3163  3509  3619  5018  5581  5599  5722  5738\n",
      "  5902  6230  6253  6579 11716 13345 15611 22813 29756 29761 30119 30261\n",
      " 35319 35517 35758 41378 41381 41398 44485 44538 55373 55375 55376 61841]\n",
      "Cleaned file saved as 20250116_TRAP2_5VLS_social_L4_TRAP2_208+209.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0        1372.0        813.0           433.0              66           1314.0\n",
      "1        5581.0        989.0           434.0              78           1322.0\n",
      "2        6253.0       1086.0           435.0              79           6233.0\n",
      "3       13345.0       3163.0           436.0             338          29786.0\n",
      "4       22813.0       3509.0           437.0             339          38875.0\n",
      "...         ...          ...             ...             ...              ...\n",
      "8255        NaN          NaN             NaN           62495              NaN\n",
      "8256        NaN          NaN             NaN           62496              NaN\n",
      "8257        NaN          NaN             NaN           62497              NaN\n",
      "8258        NaN          NaN             NaN           62498              NaN\n",
      "8259        NaN          NaN             NaN           62500              NaN\n",
      "\n",
      "[8260 rows x 5 columns]\n",
      "Loaded 101376 rows from C:/Users/jjmmc/Documents/no_cleaning/20250116_TRAP2_5VLS_social_L4_TRAP2_218+219.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 identity tracks with 54789 frames each\n",
      "Detecting gap-based identity swaps...\n",
      "[np.int64(6809), np.int64(31353)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 17 at frames: [ 5991  5992  5997  5998  6809  7216 25206 29800 29828 31279 31283 31353\n",
      " 33279 47596 52010 52282 52570]\n",
      "Cleaned file saved as 20250116_TRAP2_5VLS_social_L4_TRAP2_218+219.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0        6809.0       5991.0              30           990.0           2158.0\n",
      "1       31353.0       5992.0              31           991.0           6837.0\n",
      "2           NaN       5997.0              34          1032.0          13569.0\n",
      "3           NaN       5998.0              47          1033.0          37867.0\n",
      "4           NaN       7216.0             816          1034.0          46920.0\n",
      "...         ...          ...             ...             ...              ...\n",
      "4091        NaN          NaN           54631             NaN              NaN\n",
      "4092        NaN          NaN           54632             NaN              NaN\n",
      "4093        NaN          NaN           54633             NaN              NaN\n",
      "4094        NaN          NaN           54634             NaN              NaN\n",
      "4095        NaN          NaN           54741             NaN              NaN\n",
      "\n",
      "[4096 rows x 5 columns]\n",
      "Loaded 62719 rows from C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_205+206.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 identity tracks with 34346 frames each\n",
      "Detecting gap-based identity swaps...\n",
      "[np.int64(5679), np.int64(10030), np.int64(32781)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 34 at frames: [  452   456  2963  2965  3787  3821  3936  5679  8699  8700  9424 10030\n",
      " 10051 11494 11495 13240 15485 15486 17103 17158 17232 17265 17284 17286\n",
      " 17288 17289 21849 22196 22207 22208 22209 22674 32779 32781]\n",
      "Cleaned file saved as 20250117_TRAP2_5VLS_social_L5_TRAP2_205+206.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0        5679.0        452.0           446.0              30           1885.0\n",
      "1       10030.0        456.0           447.0              33           5608.0\n",
      "2       32781.0       2963.0           448.0              36           5609.0\n",
      "3           NaN       2965.0           449.0             239           9420.0\n",
      "4           NaN       3787.0           450.0             251          11496.0\n",
      "...         ...          ...             ...             ...              ...\n",
      "2456        NaN          NaN             NaN           34282              NaN\n",
      "2457        NaN          NaN             NaN           34283              NaN\n",
      "2458        NaN          NaN             NaN           34284              NaN\n",
      "2459        NaN          NaN             NaN           34301              NaN\n",
      "2460        NaN          NaN             NaN           34323              NaN\n",
      "\n",
      "[2461 rows x 5 columns]\n",
      "Loaded 60357 rows from C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_208+209.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 identity tracks with 36210 frames each\n",
      "Detecting gap-based identity swaps...\n",
      "[np.int64(934), np.int64(940), np.int64(7219), np.int64(8091), np.int64(21281), np.int64(33625)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 36 at frames: [  934   937   938   939   940  1941  4030  4044  4228  7219  7915  7923\n",
      "  8091  8397  8692 10448 10479 11577 11579 15231 15233 15234 15254 19473\n",
      " 20674 21281 21287 21299 21495 21496 22125 27531 31281 33625 33672 33990]\n",
      "Cleaned file saved as 20250117_TRAP2_5VLS_social_L5_TRAP2_208+209.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0         934.0        937.0             1.0             139           8253.0\n",
      "1         940.0        938.0             5.0             164           8254.0\n",
      "2        7219.0        939.0             6.0             165          11276.0\n",
      "3        8091.0       1941.0             7.0             166          17962.0\n",
      "4       21281.0       4030.0             8.0             167          17963.0\n",
      "...         ...          ...             ...             ...              ...\n",
      "4392        NaN          NaN             NaN           36205              NaN\n",
      "4393        NaN          NaN             NaN           36206              NaN\n",
      "4394        NaN          NaN             NaN           36207              NaN\n",
      "4395        NaN          NaN             NaN           36208              NaN\n",
      "4396        NaN          NaN             NaN           36209              NaN\n",
      "\n",
      "[4397 rows x 5 columns]\n",
      "Loaded 78962 rows from C:/Users/jjmmc/Documents/no_cleaning/20250117_TRAP2_5VLS_social_L5_TRAP2_218+219.csv\n",
      "Created 2 identity tracks with 41254 frames each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjmmc\\AppData\\Local\\Temp\\ipykernel_71404\\3979579838.py:231: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting gap-based identity swaps...\n",
      "[np.int64(2502), np.int64(9935), np.int64(23454), np.int64(36404)]\n",
      "Detecting normal identity swaps...\n",
      "Total swaps detected: 30 at frames: [ 1938  1939  1940  2094  2502  2504  5704  5744  6839  6852  7562  9935\n",
      " 12501 13897 14966 14968 14980 15916 16762 23448 23453 23454 25320 25322\n",
      " 25328 25329 35036 36233 36404 38539]\n",
      "Cleaned file saved as 20250117_TRAP2_5VLS_social_L5_TRAP2_218+219.csv\n",
      "      swap_gaps  swap_normal  collisions_id1  collisions_id2  collisions_both\n",
      "0        2502.0       1938.0            53.0             120           7106.0\n",
      "1        9935.0       1939.0            54.0             122           7109.0\n",
      "2       23454.0       1940.0            55.0             123           7703.0\n",
      "3       36404.0       2094.0            56.0             124          26252.0\n",
      "4           NaN       2504.0           341.0             125              NaN\n",
      "...         ...          ...             ...             ...              ...\n",
      "1635        NaN          NaN             NaN           38992              NaN\n",
      "1636        NaN          NaN             NaN           38993              NaN\n",
      "1637        NaN          NaN             NaN           38994              NaN\n",
      "1638        NaN          NaN             NaN           38995              NaN\n",
      "1639        NaN          NaN             NaN           40503              NaN\n",
      "\n",
      "[1640 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# double_identity_tracker(5)\n",
    "for i in range(len(Doubles)):\n",
    "    double_identity_tracker(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64715fb-38fe-4825-99a2-4e975bac1116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f163a4-ac33-4cae-b24e-ba09929708a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
